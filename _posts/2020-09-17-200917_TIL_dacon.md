---
title: 200917_TIL_dacon
categores: NLP
tags: NLP, dacon
last_modified_at: 2020-09-17 21:44:00 -0500
---

### 오늘 한 일

* 9월 셋째주 목표는 Dacon이 제공하는 데이터를 활용해 실습해보는 것. 9월 16일, 17일 이틀을 걸쳐 해보았다.


* [Dacon 청와대 청원: 청원의 주제가 무엇일까?](https://dacon.io/competitions/open/235597/overview/)

  Dacon에서 제공하는 교육용 데이터셋이다. 청와대 청원 데이터로, 40000개의 train 데이터, 5000개의 test 데이터로 총 45000개의 데이터셋을 제공한다. 목표는 청원의 카테고리(인권, 성평등/ 문화,예술,체육,언론/ 육아,교육) 맞추기!

* [wikidocs 딥러닝을 이용한 자연어처리 입문](https://wikidocs.net/44249) 의 실습 코드를 참고하였다.


* 최근에 TF-IDF를 배웠기 때문에 이를 활용하기 위해 전체 문서 내 단어의 빈도수를 통해 불용어를 지정해보았다.


* 아래는 코드. colab 환경에서 실습했다.

```python
# 필요한 라이브러리 불러오기
pip install konlpy
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Embedding
from tensorflow.keras.preprocessing.text import Tokenizer
import numpy as np
import re
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from konlpy.tag import Okt
%matplotlib inline
import matplotlib.pyplot as plt
# 데이터 불러오기
train_data = pd.read_csv('/content/sample_data/train.csv')
test_data = pd.read_csv('/content/sample_data/test.csv')
# 데이터 확인해보기
train_data.head()
test_data.head()
print(train_data.shape, test_data.shape)
# 전처리: 중복값 제거
train_data['data'].nunique()
train_data.drop_duplicates(subset=['data'], inplace=True)
print(train_data.shape)
# 전처리: 결측치 제거
print(train_data.isnull().values.any())
print(train_data.isnull().sum())
train_data.loc[train_data.data.isnull()]
train_data = train_data.dropna(how = 'any')
# 결측치 없는지 확인
print(train_data.isnull().values.any())
# 데이터에 특수문자 제거(한글만 남기고 모두 없애기)
train_data['data'] = train_data['data'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")
# 확인
train_data['data'][0]
# test 데이터도
test_data['data'] = test_data['data'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")
# 전처리: 불용어 정하기. Term frequency 함수 활용
word_dict = {}
words = train_data['data']

for word in words:
    word_list = word.split()
    for w in word_list:
        word_dict[w] = 1 + (0 if word_dict.get(w) is None else word_dict[w])

TF = pd.Series(word_dict).sort_values(ascending=False)
# 데이터에서 가장 많이 나오는 단어 100개 보기
print(TF[:100])
# 내가 지정한 불용어들
stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '과', '도', 
             '를', '으로', '자', '에', '와', '한', '하다', '하지만', '수', 
             '있는', '그', '대한', '하는', '더', '있습니다', '합니다', '할', 
             '하고', '및', '저는', '많은', '이런', '년', '위해', '없는', 
             '너무', '왜', '다', '아니라', '대한민국', '우리', '현재', '대해', 
             '또', '이렇게', '것입니다', '청원합니다', '없습니다', '부탁드립니다', '하지만', '그리고', '그런데']
# 토큰화 하기
okt = Okt()
X_train = []
for sentence in train_data['data']:
    temp_X = []
    temp_X = okt.morphs(sentence, stem=True) # stem = True는 단어를 정규화시켜준다.
    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거
    X_train.append(temp_X)
    
# test 데이터도
X_test = []
for sentence in test_data['data']:
    temp_X = []
    temp_X = okt.morphs(sentence, stem=True)
    temp_X = [word for word in temp_X if not word in stopwords]
    X_test.append(temp_X)
# 데이터 확인
print(X_train.shape)
print(X_train[0])
# 정수 인코딩
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)
print(tokenizer.word_index)
len(tokenizer.word_index)
# 1번만 나온 단어는 사전에서 제거한다.(2회 미만)
threshold = 2
total_cnt = len(tokenizer.word_index) # 단어의 수
rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트
total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합
rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합

# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.
for key, value in tokenizer.word_counts.items():
    total_freq = total_freq + value

    # 단어의 등장 빈도수가 threshold보다 작으면
    if(value < threshold):
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value

print('단어 집합(vocabulary)의 크기 :',total_cnt)
print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))
print("단어 집합에서 희귀 단어의 비율:", (rare_cnt / total_cnt)*100)
print("전체 등장 빈도에서 희귀 단어 등장 빈도 비율:", (rare_freq / total_freq)*100)
# 전체 단어 개수 중 빈도수 1번인 단어 개수 제거.
# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2
vocab_size = total_cnt - rare_cnt + 2
print('단어 집합의 크기 :',vocab_size)
# 정수 인코딩
tokenizer = Tokenizer(vocab_size, oov_token = 'OOV')
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)
# 확인
print(X_train[0])
# 패딩하기
print('청원의 최대 길이 :',max(len(l) for l in X_train))
print('청원의 평균 길이 :',sum(map(len, X_train))/len(X_train))
plt.hist([len(s) for s in X_train], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()
# max_len 길이 구하는 함수
def below_threshold_len(max_len,nested_list):
    cnt = 0
    for s in nested_list:
        if(len(s) <= max_len):
            cnt = cnt + 1
    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))* 100))
# 나는 max_len=600으로 결정
max_len=600
X_train = pad_sequences(X_train, maxlen = max_len)
X_test = pad_sequences(X_test, maxlen = max_len)
# 모델 만들기
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import tensorflow as tf
# y_train 가져오기
from tensorflow.keras.utils import to_categorical
y_train = to_categorical(train_data['category'])
print(len(X_train), len(y_train))
# 학습
model = Sequential()
model.add(Embedding(vocab_size, 120))
model.add(LSTM(120))
model.add(Dense(3, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])

history = model.fit(X_train, y_train, batch_size=128, epochs=15)
# Dacon에 제출할 예측 정답 파일
y_pred = model.predict_classes(X_test)
type(y_pred)
sample_submission = pd.read_csv('/content/sample_submission.csv')
sample_submission['category'] = y_pred
sample_submission.to_csv('/content/sample_data/submission.csv', encoding='utf-8', index = False)

```

* 결과는 83점 받았다.

![dacon](assets/images/dacon.PNG){: width="50%" height="50%"}

### 느낀점

* 책이나 강의로만 공부하다가 직접 실습 코드 써보니까 지식으로만 알고 있던 것 (예: x_train과 y_train의 shape를 맞춰야 한다) 을 피부로 느낄 수 있었다. 오류에서 정말 많이 배운다... 

* 함수나 라이브러리가 요구하는 자료 구조로 변형(?)하는 게 어려웠다. 내가 머릿 속으로 상상한 거랑 다르게 계속 어긋나더라.ㅎㅎ 머릿속으로는 이미 모델 학습 다 시키고도 남았음.

* 아무튼 딥러닝 어린이인데 책에서 하라는대로 모델 학습시키니 얼추 80점은 나오니 너무 신기했다. 이래서 개발자들이 1% , 0.1% 정확도를 올리려고 노력하는구나 싶었다.

* 다음 과제는 validation dataset 만들어서 다시 모델 학습시키기. 오늘은 아침부터 달렸으므로 이만 휴식...! 
