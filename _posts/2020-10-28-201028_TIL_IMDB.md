---
title: 201028_TIL_IMDB
categories: NLP
tags: NLP IMDB
last_modified_at: 2020-10-28 22:50:00 -0500
---

## IMDB로 공부하기

* 지난 청와대 청원 분석에서 막혔던 부분은 Word2Vec으로 학습한 것을 모델에 적용하는 것이었다.

* 마침 딱 맞는(?) 튜토리얼을 찾았다.

* Kaggle IMDB 튜토리얼: https://www.kaggle.com/c/word2vec-nlp-tutorial 

* 이 모델은 텍스트 분류의 영역이고 Word2Vec을 이용해서 문제를 해결한다. 

* IMDB는 워낙 유명한 자료여서 [wikidocs](https://wikidocs.net/24586)에도, 내가 정말 좋아하는 [박조은 님의 코드](https://www.kaggle.com/kongnyooong/imdb-review-nlp-tutorial-part-1/notebook)도 kaggle에 공개되어 있다.

* 하지만 익숙한 한글 학습 자료인 박조은 님의 자료와 wikidocs 자료보다 낯선 자료로 공부를 시도해보았다.

### 공부 방법

* 1. 먼저 튜토리얼을 자세히 읽어보았다. Data 도 다운 받아서 확인해보았다.
  2. 그 중 [99%의 정확도가 나왔다는 분의 Kaggle Notebook](https://www.kaggle.com/alexcherniuk/imdb-review-word2vec-bilstm-99-acc)을 찬찬히 읽어보았다. (마치 전교 1등의 핵심 노트 훔쳐보기) 처음은 그냥 읽어보았음.
  3. IMDB 데이터에 익숙해지려고 wikidocs 문서의 코드를 따라 써보았다.
  4. 그 다음 다시 99%의 정확도 코드(일명 1등 노트)를 따라 써보았다.

* 1등 노트의 코드를 따라 써보고 이해가 안 가거나 어려운 부분은 구글링으로 찾아보고, 주석으로 부연 설명 달아보며 공부하였다. 나랑 로컬 환경도 다르고 additional data를 사용하지 않아서 내 상황에 맞게 조금씩 고쳐서 코드를 다라썼다.

* 새로 알게 된 점: 전교 1등은 그냥 모델만 학습시키는 것이 아니라 matplotlib 라이브러리를 잘 활용해서 데이터, 심지어는 모델을 만들고 나서 시각화시킨다. 그러니까 본인이 선택한 코드를 납득(?) 시킨다고 해야 하나. 임베딩 사이즈를 선택할 때도 그에 적합한 그래프를 보여준다. 물론 notebook용이지만 협업할 땐 필수인 습관 같다. 배우고 나도 적용해봐야지! 연습하자!

## 결과

* 전교 1등 노트를 따라 쓴다고 나도 1등급이 나오는 건 아니다.

![title](/assets/images/50socre.PNG)

* 50점을 받았다. :nerd_face: 

* ㅎㅎ... 중간 과정에서 잘못한 게 있나보다. 코드를 다시 보고 고치거나 나도 additional data를 구해서 학습시켜야겠다.
