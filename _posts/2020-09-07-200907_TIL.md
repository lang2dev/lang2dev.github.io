---
title: 200907_TIL
categories: TIL
tags:
last_modified_at: 2020-09-07 22:20:00 -0500
---

## 오늘 한 일


* <김기현의 자연어 처리 딥러닝 캠프> 5장 '유사성과 모호성' ~5.4까지 공부했다.


* wordnet을 활용해서 특정 단어의 최상위 부모 노드까지의 경로를 구해보는 실습을 했다. 이 함수를 활용하여, 단어 간 유사도를 구해보았다.

## 기억하기


* 오늘 배운 '중의성'은 자연어 처리에서 굉장히 중요한 문제이다. 자연어처리가 어려운 이유는 본질적으로 언어의 중의성, 이 단원에서는 주로 '단어'의 중의성이 존재하기 때문이다. 현재에는 RNN을 주로 사용하며 단어 중의성 해소 문제는 많이 해결되었지만 여전히 모호한 의미로 문제 해결이 어려운 경우가 많다. 


* **단어 간 관계 구조**를 계층화하면 자연어 처리에 유용하게 사용할 수 있다. 단어 간 유사도를 구할 수 있고 부족한 정보를 비슷한 관계의 다른 단어들로부터 얻어올 수도 있다. (155쪽)


* 원핫 인코딩의 단점: 벡터의 차원이 너무 커진데다, 벡터의 많은 부분이 0으로 채워진 희소 벡터이므로 벡터 간 연산을 할 때 결괏값이 0이 되는 게 문제! (서로 직교하는 경우가 많아진다.)  '강아지'와 '개'의 유사도도 0, '강아지'와 '컴퓨터'의 유사도도 0이 되므로 표현 방식이 매우 불리. (158쪽)


* 원핫 인코딩의 단점을 해결할 방법: 특징 벡터 이용. 특징 벡터는 각 샘플의 특징별 수치를 모아 벡터로 표현한 것이다. 즉, 각 샘플들은 특징에 따라 수치를 가지게 된다. (165쪽)

## 내일 공부


* 내일은 TF-IDF를 공부하고 실습까지 해봐야겠다!

